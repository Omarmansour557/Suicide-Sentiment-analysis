{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd23fd0",
   "metadata": {},
   "source": [
    "------------------\n",
    " ## Suicide Sentiment Analysis Project \n",
    " - Using TF-IDF As Feature Extraction\n",
    " - Using Some Classification models As RandomForest, LinearSVC, MultinomialNB\n",
    " - Using Some Preprocessing as Lemmatization, Removing Stop Words\n",
    " - Finally The Results are around 87% which is good. \n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "758e5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d869b",
   "metadata": {},
   "source": [
    "## Read Suicide_Detection File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "631012bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Suicide = pd.read_csv('Suicide_Detection.csv')\n",
    "data_split = np.array_split(Suicide, 20)\n",
    "Suicide = data_split[0]\n",
    "Suicide = Suicide.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800780a5",
   "metadata": {},
   "source": [
    "## Preparing For Stopword removal and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c3e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/omar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27d01e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Suicide.drop('class', axis=1)\n",
    "y = Suicide['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca734e9f",
   "metadata": {},
   "source": [
    "# Text Pre Proceessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b3f534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove emails\n",
    "email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    # removing all special charachter\n",
    "    review = re.sub('[^a-zA-Z]', ' ', str(X['text'][i]))\n",
    "    # make document as lowerCase\n",
    "    review = review.lower()\n",
    "    # splitting the documents into words for ex ['iam', 'omar']\n",
    "    review = review.split()\n",
    "    # make limmatization --> (change, changing, changes)---> (change)\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords)]\n",
    "    # join the document agian\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # removing mails\n",
    "    for r in regexes_to_remove:\n",
    "        X['text'][i] = re.sub(r, '', review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1de76d",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2b883ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101ce49",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a785d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train['text'])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39928e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8122, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffa13b",
   "metadata": {},
   "source": [
    "---------------\n",
    "- As we see the no. of features very large so we need to make feature selection and feature scaling\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddaf19",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04f01fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# function to fit data(calculate the min and max) then transform data to it\n",
    "X_norm = scaler.fit_transform(X_tfidf_train.toarray())\n",
    "X_test_norm = scaler.transform(X_tfidf_test.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8f936",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7808061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# We Can select any model but linearSVC has l1 norm penality which deals with sparse\n",
    "lsvc = LinearSVC(C=100, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(X_norm, y_train)\n",
    "\n",
    "# This function select the best features that has high weigh\n",
    "fs = SelectFromModel(lsvc, prefit=True)\n",
    "# This function redeuce X to the selected features\n",
    "X_selection = fs.transform(X_norm)\n",
    "X_test_selection = fs.transform(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96af4c",
   "metadata": {},
   "source": [
    "##  Using LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00aa04da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3482,)\n",
      "(3482,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Suicide       0.86      0.84      0.85      1728\n",
      " Non-Suicide       0.85      0.86      0.86      1754\n",
      "\n",
      "    accuracy                           0.85      3482\n",
      "   macro avg       0.85      0.85      0.85      3482\n",
      "weighted avg       0.85      0.85      0.85      3482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(X_selection, y_train)\n",
    "y_predict_1 = lsvc.predict(X_test_selection)\n",
    "print(metrics.classification_report(y_test, y_predict_1, target_names=['Suicide', 'Non-Suicide']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7684383",
   "metadata": {},
   "source": [
    "## Using RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b1d3843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=10)\n",
    "clf.fit(X_selection, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c4b3544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Suicide       0.79      0.91      0.85      1728\n",
      " Non-Suicide       0.89      0.77      0.83      1754\n",
      "\n",
      "    accuracy                           0.84      3482\n",
      "   macro avg       0.84      0.84      0.84      3482\n",
      "weighted avg       0.84      0.84      0.84      3482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_2 = clf.predict(X_test_selection)\n",
    "print(metrics.classification_report(y_test, y_predict_2, target_names=['Suicide', 'Non-Suicide']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6dc68",
   "metadata": {},
   "source": [
    "## Using Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cea77e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Suicide       0.79      0.91      0.85      1728\n",
      " Non-Suicide       0.89      0.77      0.83      1754\n",
      "\n",
      "    accuracy                           0.84      3482\n",
      "   macro avg       0.84      0.84      0.84      3482\n",
      "weighted avg       0.84      0.84      0.84      3482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mb = MultinomialNB()\n",
    "mb.fit(X_selection, y_train)\n",
    "y_predict_3 = clf.predict(X_test_selection)\n",
    "print(metrics.classification_report(y_test, y_predict_3, target_names=['Suicide', 'Non-Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2670d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
